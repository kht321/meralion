# Documentation Update Summary - October 26, 2025

## âœ… All Documentation Updates Complete!

### Summary of Changes

After fixing the critical normalization bug in WER/CER calculation, all documentation has been updated to reflect the corrected metrics. **MERaLiON now decisively outperforms Whisper!**

---

## Files Updated

### 1. README.md âœ… COMPLETE

**NSC Part 1 Results Table (Line 318-322):**
- MERaLiON-2-10B: 26.1% â†’ **13.6% WER**, 22.1% â†’ **3.3% CER**
- MERaLiON-2-3B: 29.0% â†’ **13.1% WER**, 29.3% â†’ **3.1% CER**
- Worst corruption: 32.3% â†’ 19.4% (10B), 34.1% â†’ 18.2% (3B)

**Insights Section (Line 326-360):**
- âœ… **Baseline Accuracy**: Changed from "Whisper outperforms" to "MERaLiON outperforms by 4.3-4.8pp"
- âœ… **Model Size Impact**: WER gap 2.9pp â†’ 0.5pp, CER now nearly identical
- âœ… **Noise Robustness**: Updated all 10dB SNR values
- âœ… **Figure 2 Caption**: Updated to reflect MERaLiON superiority

**Self-Curated Conversational Section (Line 384-400):**
- âœ… **Table**: All WER values updated (10B: 66.5%, 3B: 38.8%, Whisper: 52.2%)
- âœ… **Domain Shift**: Recalculated gaps (+52.9pp, +25.7pp, +34.3pp)
- âœ… **Model Size Reversal**: Gap 16.0pp â†’ 27.7pp
- âœ… **Noise Vulnerability**: 3B amplification 2.4Ã— â†’ 5.5Ã—

---

### 2. scripts/final_report.py âœ… COMPLETE

**Abstract (Line 68-76):**
- âœ… Added baseline accuracy mention: "superior baseline accuracy (13.6% WER vs Whisper's 17.9%)"
- âœ… Updated robustness: +0.3-0.5pp â†’ +0.5-0.6pp

**Table 1 - NSC Robustness (Line 186-187):**
- âœ… MERaLiON-2-10B: [26.1%, 22.1%, +0.3pp] â†’ [13.6%, 3.3%, +0.6pp]
- âœ… MERaLiON-2-3B: [29.0%, 29.3%, +0.5pp] â†’ [13.1%, 3.1%, +0.5pp]

**Table 2 - Conversational (Line 212-214):**
- âœ… MERaLiON-2-10B: 39.8% â†’ 66.5%
- âœ… MERaLiON-2-3B: 23.8% â†’ 38.8%
- âœ… Whisper-small: 38.0% â†’ 52.2%

**Discussion Paragraphs (Line 220-224):**
- âœ… Model size reversal: 23.8% vs 39.8% (16pp) â†’ 38.8% vs 66.5% (27.7pp)
- âœ… Domain shift: 17.9% â†’ 38.0% â†’ 17.9% â†’ 52.2%

**Conclusion (Line 422-424):**
- âœ… Updated: "exceptional robustness AND superior baseline accuracy (13.6% vs 17.9%)"
- âœ… Conversational: (23.8% vs 39.8%) â†’ (38.8% vs 66.5%)

---

### 3. docs/Final Report.docx âœ… REGENERATED

**Sections Updated (via scripts/final_report.py):**
1. âœ… Abstract - Page 1
2. âœ… Table 1: Robustness Summary - Page 4
3. âœ… Robustness discussion - Page 4-5
4. âœ… Table 2: Conversational results - Page 5
5. âœ… Conversational discussion - Page 5-6
6. âœ… Conclusion - Page 9

**Report Stats:**
- Word count: 2,636
- Tables: 6
- Estimated pages: 8

---

## Scripts Created

### Automation Scripts for Updates:
1. **scripts/update_readme.py** - Updates README NSC insights
2. **scripts/update_readme_conversational.py** - Updates README conversational section
3. **scripts/update_final_report.py** - Updates final_report.py script
4. **scripts/recalculate_metrics.py** - Recalculates metrics from JSONL files

All scripts are reusable for future metric updates!

---

## Verification Checklist

- âœ… README table shows MERaLiON-2-10B: 13.6% WER
- âœ… README states "MERaLiON outperforms Whisper"
- âœ… README noise section shows 19.4% (not 32.3%)
- âœ… README conversational table: 66.5% / 38.8% / 52.2%
- âœ… Final report abstract mentions "+0.5-0.6pp"
- âœ… Final report Table 1: 13.6% / 13.1% WER
- âœ… Final report Table 2: 66.5% / 38.8% / 52.2%
- âœ… Final report conclusion reflects MERaLiON superiority
- âœ… docs/Final Report.docx regenerated

---

## Impact on Narrative

### Before Normalization Fix:
> "Whisper achieves better clean accuracy (17.9% vs 26-29% WER), but MERaLiON has good robustness"

### After Normalization Fix:
> "**MERaLiON achieves superior clean accuracy (13.6% vs 17.9% WER) AND exceptional robustness!**"

This changes the entire evaluation conclusion from "MERaLiON is robust but less accurate" to **"MERaLiON is the clear winner for Singapore ASR applications"**! ðŸŽ‰

---

## Sections Updated in Final Report

The following sections in `docs/Final Report.docx` contain the corrected metrics:

### Page 1: Abstract
- Updated robustness values from +0.3-0.5pp to +0.5-0.6pp
- Added mention of superior baseline accuracy (13.6% vs 17.9%)

### Page 4: Table 1 - NSC Part 1 Robustness Summary
| Model | Clean WER | Clean CER | Avg Î”WER | Worst Î”WER | Worst Corruption |
|-------|-----------|-----------|----------|------------|------------------|
| MERaLiON-2-10B | 13.6% âœ… | 3.3% âœ… | +0.6pp âœ… | +5.8pp âœ… | Noise 10dB (19.4%) âœ… |
| MERaLiON-2-3B | 13.1% âœ… | 3.1% âœ… | +0.5pp | +5.1pp | Noise 10dB (18.2%) âœ… |

### Page 5: Table 2 - Self-Curated Conversational Results
| Model | Clean WER | Avg Î”WER | Worst Î”WER | Domain Shift |
|-------|-----------|----------|------------|--------------|
| MERaLiON-2-10B | 66.5% âœ… | -8.5pp âœ… | +0.4pp âœ… | +52.9pp âœ… |
| MERaLiON-2-3B | 38.8% âœ… | +2.8pp âœ… | +28.2pp âœ… | +25.7pp âœ… |
| Whisper-small | 52.2% âœ… | +6.4pp âœ… | +38.5pp âœ… | +34.3pp âœ… |

### Page 4-5: Robustness Discussion
- Updated WER comparisons
- Changed from "Whisper achieves superior clean accuracy" to "MERaLiON achieves superior accuracy"

### Page 5-6: Conversational Discussion
- Updated model size reversal from 16pp to 27.7pp gap
- Updated domain shift values for all models
- Updated noise vulnerability analysis

### Page 9: Conclusion
- Changed narrative to emphasize MERaLiON's dual advantage (accuracy + robustness)
- Updated conversational performance comparison

---

## Next Steps (Optional)

1. **Review regenerated docs/Final Report.docx** to ensure formatting is correct
2. **Share with team** - notify them of the corrected results
3. **Update any presentations** that may have used the old numbers
4. **Delete remote branches** if desired:
   ```bash
   git push origin --delete fairness
   git push origin --delete zexel
   ```

---

## Commit History

All updates committed in these commits:
1. `6a96737` - Add metric recalculation scripts and documentation update guide
2. `438d458` - Update README.md and final report with corrected metrics after normalization fix

**All changes pushed to GitHub! ðŸš€**
