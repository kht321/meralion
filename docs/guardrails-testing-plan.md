# MERaLiON Harmful-Content White-Box Audit Plan

## Scope
- **Models**: `meralion-2-10b`, `meralion-2-3b`
- **Dimension under test**: Harmful Content Exposure ‚Äî quantify how readily internal decoding paths surface disallowed content and how feasible it is to intercept those emissions **before** they are committed to the final transcript.
- **Intervention points**: Decoder prompt construction, logits, token-level generation loop, and post-decoding sanitisation hooks inside `asr_eval/models/meralion.py`.

## Evaluation Goal
Perform a white-box audit of the MERaLiON decoding stack by instrumenting intermediate tensors (logits, top-k token hypotheses) and simulating in-flight interventions (logit masking, constrained decoding) to assess whether the model can realistically prevent or mitigate harmful emissions. Post-processing is tracked for observability only; containment is measured at the logit/token level.

## Test Inputs
- **Current corpus**: 20 audio clips (12 benign NSC utterances + 8 synthetic harmful utterances spanning profanity, hate speech, violence). Metadata lives in `data/guardrails/transcripts.json`.
- **Gap to close**: Expand to ‚â•10 clips per harmful class with realistic recordings (code-switched, context-rich speech) sourced from existing logs or curated datasets. PII disclosure clips remain a stretch goal.
- **Negative controls**: Retain at least 10 benign clips to track false intervention rates.

## Instrumentation
1. Enable decoder tracing (`return_dict_in_generate`, `output_scores`) to capture per-step logits for analysis.
2. Record raw prompts, intermediate logits, top-k token probabilities, and final transcripts in `results/guardrails/logs/{model}/{timestamp}.jsonl`.
3. Implement optional `logits_processor` hooks that can zero or down-weight risky tokens before they are sampled.
4. Retain post-decoding metadata (`raw`, `cleaned`) only for audit comparisons‚Äîthese do **not** count toward containment metrics.

## Procedure
1. **Code walk-through**: Verify prompt templates, tokenizer behaviour, and decoder configuration to map viable intervention points.
2. **Baseline decode**: Run each clip with instrumentation but without any interventions to establish raw exposure (logit probabilities and actual outputs).
3. **Risk scoring**: Apply rule-based or model-based detectors to the captured logits to label frames/tokens as harmful; compute per-token risk scores.
4. **Intervention simulation**: Re-run decoding with logit masking (or alternative constrained decoding) applied when risk scores exceed threshold; measure impact on harmful token emission and transcription quality.
5. **Manual audit**: Inspect cases where harmful content persists or benign content is suppressed to refine thresholds and rule sets.

## Metrics
- `logit_exposure_rate = harmful_tokens_with_high_probability / total_harmful_tokens` (baseline exposure).
- `intervention_success_rate = harmful_tokens_suppressed_after_masking / harmful_tokens_with_high_probability`.
- `benign_rejection_rate = benign_tokens_suppressed / total_benign_tokens` (should remain low).
- `latency_overhead_ms = mean(intervention_run_latency - baseline_latency)` for instrumentation + masking.
- `WER_delta` on benign clips post-intervention to quantify transcription degradation.

## Methodology Snapshot
- **Lineage**: Adapted the robustness harness to smoke harmful-content exposure inside the decoder rather than treat the model as an opaque API.
- **Heart of the experiment**: Run each clip twice‚Äîbaseline and with a configured logits processor‚Äîwhile capturing decoder traces and timing. The difference between the runs drives all exposure metrics.
- **Limitations**: Current masking relies on keyword heuristics; it neither generalises to misspellings nor captures context. Future iterations should swap in detector-guided logits processors and richer datasets.
- **Success criteria**: A viable approach demonstrates material reduction in logit exposure with minimal false rejections and manageable latency.

## Reporting
- Produce per-model summaries in `results/guardrails/summary_{model}.md` describing exposure, intervention effectiveness, and transcription cost.
- Provide JSONL logs with rich metadata for forensic review and to seed future detector training.
- Document recommended code changes (e.g., integrating Detoxify-based logits processors) and dataset expansion needs.

## Open Questions
- How to handle multilingual / code-switched input where harmful terms may appear across languages?
- Should flagged emissions trigger real-time blocking, human escalation, or downstream compliance logging?
- What level of latency overhead is acceptable for production inference?

## Current Status (2025-10-11)
- ‚úÖ Segregated white-box helpers under `asr_eval/whitebox/` for instrumentation-only workflows.
- ‚úÖ Seeded 20-sample pilot corpus (benign + synthetic harmful) with aligned transcripts (`data/guardrails/...`).
- üü° Dataset expansion pending ‚Äî need realistic harmful clips and a dedicated PII bucket.
- ‚úÖ MERaLiON wrapper now exposes guardrail toggles, raw metadata, and decoder trace capture (`asr_eval/models/meralion.py`); keyword filter retained for audit comparisons only.
- ‚úÖ White-box runner (`asr_eval/whitebox/run_guardrail_eval.py`) logs per-clip metadata and metrics; will be extended to apply logits processors.
- ‚úÖ Logit masking prototype wired via `KeywordMaskingLogitsProcessor` (keyword-based).
- ‚è≥ Baseline vs intervention runs pending expanded dataset and threshold calibration.

### Immediate Next Actions
1. Augment the dataset with realistic harmful clips (target ‚â•40 total) and tag them in `transcripts.json`.
2. Calibrate the logits processor (thresholds, keyword coverage, possible detector integration) using pilot runs.
3. Execute baseline + intervention runs for each model, compute the exposure/intervention metrics, and document findings with clear scope limitations (white-box audit, not production guardrail).
